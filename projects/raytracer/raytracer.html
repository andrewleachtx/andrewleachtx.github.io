<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Andrew Leach | Raytracing</title>
    <link rel="icon" href="../../assets/favicon.png" type="image/x-icon" />
    <meta name="description" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="../../style.css" />
    <link rel="stylesheet" href="../../mediaqueries.css" />

    <!-- MathJax -->
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>

  <body>
    <html>
      <nav id="desktop-nav">
        <div class="logo">Andrew D. Leach</div>

        <div>
          <ul class="nav-links">
            <li>
              <a href="../../">
                <img
                  src="../../assets/icon-home.svg"
                  alt="home icon"
                  class="icon-grow"
                />
              </a>
            </li>
            <li><a href="../../#about">me</a></li>
            <li><a href="../../#experience">experience</a></li>
            <li><a href="../../#contact">contact</a></li>
            <li><a href="../../pages/projects.html">projects</a></li>
            <li><a href="../../pages/fun.html">fun</a></li>
          </ul>
        </div>
      </nav>

      <nav id="hamburger-nav">
        <div class="logo">Andrew D. Leach</div>
        <div class="hamburger-menu">
          <div class="hamburger-icon" onclick="toggleMenu()">
            <span></span>
            <span></span>
            <span></span>
          </div>
          <div class="menu-links">
            <li><a href="../../#about" onclick="toggleMenu()">about</a></li>
            <li>
              <a href="../../#experience" onclick="toggleMenu()">experience</a>
            </li>
            <li><a href="../../#contact">contact</a></li>
            <li>
              <a href="../../pages/projects.html" onclick="toggleMenu()"
                >projects</a
              >
            </li>
            <li>
              <a href="../../pages/fun.html" onclick="toggleMenu()">fun</a>
            </li>
          </div>
        </div>
      </nav>

      <div class="project-sidebar">
        <a href="#raytracer">Ray Tracing</a>
        <a href="#rasterization">Rasterization</a>
        <a href="#what-is-raytracing">Ray Tracing In 3 Steps</a>
        <a href="#c++-implementation">C++ Implementation</a>
        <a href="#scenes">Scene Images</a>
        <a href="#threading">Threading & Optimizations</a>
        <a href="#bounding-volumes">Bounding Volume Problem</a>
        <a href="#anti-aliasing">Anti-Aliasing</a>
        <a href="#csg">Constructive Solid Geometry</a>
        <a href="#future">Future & Improvements</a>
        <a href="#credits">Thanks & Credits</a>
      </div>

      <section id="raytracer" class="project">
        <h3 class="project-title">Ray Tracing</h3>

        <p>
          Growing up playing video games and fantasizing about one day building
          my dream computer with only the most outrageously powerful parts, the
          phrase
          <a
            href="https://en.wikipedia.org/wiki/Ray_tracing_(graphics)"
            target="_blank"
            >ray tracing</a
          >
          was simple: it could make games look <em>real</em> - like you were
          were looking through a window, not a screen. (It also meant really,
          really expensive).
        </p>

        <p>
          After my introduction to graphics, the way I would now describe ray
          tracing to my friends is as follows:
          <strong>you already know how to ray trace</strong>. That is to say, it
          is the intuitive way to render a scene; ray tracing replicates what
          our eyes and brains do
          <a
            href="https://www.sighthound.com/blog/human-eye-fps-vs-ai-why-ai-is-better"
            target="_blank"
            >60 times every second</a
          >.
        </p>

        <div class="project-img-container">
          <img src="./assets/scene.png" alt="My Scene" class="project-img" />
          <div>Scene 9</div>
        </div>

        <p>
          I am sure there are many nuances to my explanation of ray tracing.
          This serves as a blog and documentation for my logic and
          implementation in C++ for a ray tracer. Because this was done in my
          graphics course (with the guidance of
          <a href="https://people.engr.tamu.edu/sueda/index.html"
            >Dr. Shinjiro Sueda</a
          >), the codebase is not public.
        </p>

        <h2 id="rasterization">Rasterization</h2>
        <p>
          Before starting, I should note that ray tracing is not a new concept -
          despite recent popularity in the gaming industry over the past decade,
          it has been around since the 1960s. Why is it popular now? Well, I
          think you'll be able to come up with an answer yourself.
        </p>
        <p>
          There must then be other methods responsible for the feats of
          real-time graphics over the past ~30 years. The most well-known
          strategy in the conversation, adjacent to ray tracing, is
          <a
            href="https://blogs.nvidia.com/blog/whats-difference-between-ray-tracing-rasterization/#attachment_39178"
            target="_blank"
            >rasterization</a
          >. If a <strong>raster</strong> is a group of pixels in a 2D grid,
          then <strong>rasterization</strong> is the process by which we convert
          3D shapes into pixels on your (2D) screen.
        </p>
        <p>
          There is a lot of rasterization lore that is tempting to dive into,
          but lets assume you've already read up on it (hopefully soon I can
          write up docs for my
          <a href="../../projects/rasterizer/rasterizer.html" target="_blank"
            >rasterizer project</a
          >).
        </p>

        <h2 id="what-is-raytracing">Ray Tracing In 3 Steps</h2>
        <div class="project-img-container">
          <img
            class="project-img"
            src="./assets/frustum.png"
            alt="Screenshot from Michael Stewart"
            style="width: 100%; height: 400px"
          />
          <div>
            Screenshot taken from Michael Stewart's amazing ray tracing
            <a
              href="https://michaelstewart2.github.io/RayTeaching1.1/index.html"
              target="_blank"
              >visualizer</a
            >
          </div>
        </div>

        <p>
          Imagine your eyes shoot <em>infinitely</em> many rays out to every
          possible object in the world around you. The watermelon slices on your
          counter, your floor, your tortoise, etc. Well, when one of those rays
          reaches or <strong>intercepts</strong> with the watermelon, you see
          green and red (unless you collect rare watermelons).
        </p>

        <p>
          Well imagine that watermelon, like any other object, is composed of
          tiny spots of color that combine to make the pattern you recognize as
          a watermelon. Each ray your eye shoots
          <strong>hits a unique spot</strong>, and bounces to any nearby light
          sources to ascertain how "bright" that green spot should be. That is
          what happens for one pixel in ray tracing.
        </p>

        <p>
          Now, ray tracing says that in front of your eyes there is a 2D grid,
          known as the
          <a
            href="https://help.agi.com/STKComponents/html/GraphicsCameraViewFrustum.htm"
            target="_blank"
            >view frustum</a
          >. In our case, this view frustum is composed of evenly distributed
          pixels. Sounds familiar? That's because you're reading this off of
          one.
        </p>

        <p>
          Now, we <strong>always</strong> have a scene in front of us. Maybe a
          tortoise, some trees, a house, whatever. We want to transform that 3D
          scene into pixels on a screen. Now the question is,
          <strong
            >what colors do we give to each pixel so that when combined on the
            screen, they tell our brain there is a tortoise?</strong
          >
        </p>

        <pre><code class="language-python">for pixel in image:
            1. generate ray
            2. find closest object that intersects with ray
            3. compute color of pixel based on light(s)</code></pre>

        $$\vec{x}(t) = \vec{p} + t\hat{v}$$

        <p>
          A ray has an origin \(\vec{p}\), and a direction \(\hat{v}\). At a
          high level ray tracing argues that if for each pixel you shoot a ray
          from the eye (camera) through the pixel's center, you will
          <em>eventually</em> hit an object in the scene. At that intersection
          point, you can determine the intensity of light with a typical
          <a
            href="https://en.wikipedia.org/wiki/Blinn%E2%80%93Phong_reflection_model"
            target="_blank"
            >Blinn Phong</a
          >
          model.
        </p>

        <p>
          Basically, you get the color of the object at that point, and the
          degree to which that color is accentuated depends on the light
          sources.
          <strong>The pixel receives that final color.</strong>
        </p>

        <div class="project-img-container">
          <img
            class="project-img"
            src="assets/tracingbasics.png"
            alt="Tracing Basic PNG"
            style="width: 50%; height: auto"
          />
          <div>
            Ray trace diagram,
            <a
              href="https://en.wikipedia.org/wiki/Ray_tracing_(graphics)#/media/File:Ray_trace_diagram.svg"
              target="_blank"
              >Wikipedia</a
            >
          </div>
        </div>

        <p>
          For example, this sphere became an orange ball on the frustum because
          a collection of rays from the camera intersected with the sphere at
          those orange points. The other rays hit perhaps white planes (the
          ground and walls) and became white, or gray (shadows) for ones that
          were occluded from the light's perspective.
        </p>

        <h2 id="c++-implementation">C++ Implementation</h2>
        <p>
          I wrote this ray tracer in vanilla C++17, primarily relying on STL
          libraries. The only supplementary libraries I used are:
        </p>

        <ul>
          <li>
            <a href="https://github.com/g-truc/glm" target="_blank">glm</a> to
            abstract matrix math
          </li>
          <li>
            <a
              href="https://github.com/nothings/stb/blob/master/stb_image_write.h"
              target="_blank"
              >stb_image_write.h</a
            >
            to export pixel data into an image
          </li>
          <li>
            <a
              href="https://github.com/tinyobjloader/tinyobjloader"
              target="_blank"
              >tinyobjloader</a
            >
            to load any .obj meshes (such as the bunny in scenes 6-7)
          </li>
          <li>
            <a
              href="https://fileadmin.cs.lth.se/cs/Personal/Tomas_Akenine-Moller/raytri/"
              target="_blank"
            >
              raytri.c</a
            >
            for optimized ray-triangle intersection tests within meshes
          </li>
        </ul>

        <p>
          Setting theory aside, actually implementing the ray tracer required a
          lot of object-oriented design and refactoring of code. Most
          abstractions consisted of one or more glm::vec3's, namely ones that
          held (x, y, z) information. To name some classes I ended up with:
        </p>

        <ul>
          <li>
            <strong>Ray</strong> to store XYZ values for a ray's origin and
            direction
          </li>
          <li>
            <strong>Light</strong> to define position and intensity for light
            sources
          </li>
          <li>
            <strong>Camera</strong> for storing eye position + translations,
            aspect ratio, fovy, etc
          </li>
          <li>
            <strong>Material</strong> to store the unique diffuse and specular
            colors/floats a specific object should have
          </li>
          <li>
            <strong>Shape</strong> with polymorphism / inheritance as the
            primary goal for multiple shape types and their unique intersection
            and hit methods
          </li>
          <li>
            <strong>Scene</strong> as a wrapper container for specific instances
            of lights and shapes
          </li>
          <li>
            <strong>Mesh</strong> to store vectorized information about OBJ data
          </li>
        </ul>

        <h2 id="scenes">Scenes</h2>
        <p>
          These scenes can be rendered at variable resolution (within reason);
          anti-aliasing samples per pixel; and parallel multithreaded chunk
          counts. Rendering was done on the CPU as my existing code does not
          leverage parallelization through CUDA or any other GPU frameworks.
        </p>

        <p>
          This did not serve to be problematic given the simplicity of the
          scenes and efficiency of the code (especially with std::thread on the
          CPU).
        </p>

        <p>
          You may note some scenes are listed in pairs - this is because certain
          scenes were designed incrementally; the latter is presented. Scenes
          1-8 were rendered in 512x512 without anti-aliasing on. Scene 9,
          however, was rendered in 1024x1024 with 8x AA sampling per pixel.
        </p>

        <div class="project-img-group">
          <div class="project-img-list">
            <img
              src="./assets/scene1.png"
              alt="Scene 1"
              class="project-img-scene"
            />
            Scene 1 (no shadows) and 2 (shadows)
          </div>

          <div class="project-img-list">
            <img
              src="./assets/scene3.png"
              alt="Scene 3"
              class="project-img-scene"
            />
            Scene 3 (ellipsoid)
          </div>

          <div class="project-img-list">
            <img
              src="./assets/scene4.png"
              alt="Scene 4"
              class="project-img-scene"
            />
            Scene 4 (singular reflection) and 5 (recursive reflection)
          </div>

          <div class="project-img-list">
            <img
              src="./assets/scene6.png"
              alt="Scene 6"
              class="project-img-scene"
            />
            Scene 6 (triangle-ray intersection for .obj mesh)
          </div>

          <div class="project-img-list">
            <img
              src="./assets/scene7.png"
              alt="Scene 7"
              class="project-img-scene"
            />
            Scene 7 (triangle-ray intersections with model transformation)
          </div>

          <div class="project-img-list">
            <img
              src="./assets/scene8.png"
              alt="Scene 8"
              class="project-img-scene"
            />
            Scene 8 (camera transformation of Scene 1)
          </div>

          <div class="project-img-list">
            <img
              src="./assets/scene.png"
              alt="Scene 9"
              class="project-img-scene"
            />
            Scene 9 (anti-aliasing, reflective blinn-phong, constructive solid
            geometry, multithreading, and boxes).
          </div>
        </div>

        <h2 id="threading">Multithreading & Render Optimization</h2>
        <p>
          The ray tracing algorithm performs a lot of work in series: iterate
          over all pixels, send rays that may even recursively spawn more rays,
          etc... It is a very <strong>computationally expensive</strong>
          process, especially for high resolution images where \(rays \times
          width \times height\) grows rapidly.
        </p>

        <p>
          Yet, this work for one pixel is independent of another; concurrency is
          not a problem, and this nature lends ray tracing to parallelization.
          With some simple changes, I was able to leverage
          <a
            href="https://en.cppreference.com/w/cpp/thread/thread"
            target="_blank"
            >std::thread</a
          >
          to split up the processing for groups of pixels into
          <strong>chunks</strong>, and join them at the end.
        </p>

        <p>
          The following data was collected at 1024x1024, 1 ray per pixel (no
          AA), and 30 max recursive bounces for reflections. The series
          generation is based on 1 thread (normal), and for parallel 16 threads.
        </p>

        <table class="performance-table">
          <thead>
            <tr>
              <th>Scene</th>
              <th>Resolution</th>
              <th>Series Runtime (seconds)</th>
              <th>Parallel Runtime (seconds)</th>
              <th>Improvement (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>1024x1024</td>
              <td>0.367059</td>
              <td>0.149398</td>
              <td>59.30%</td>
            </tr>
            <tr>
              <td>2</td>
              <td>1024x1024</td>
              <td>0.362205</td>
              <td>0.166128</td>
              <td>54.13%</td>
            </tr>
            <tr>
              <td>3</td>
              <td>1024x1024</td>
              <td>0.629310</td>
              <td>0.227803</td>
              <td>63.80%</td>
            </tr>
            <tr>
              <td>4</td>
              <td>1024x1024</td>
              <td>3.479643</td>
              <td>0.314233</td>
              <td>90.97%</td>
            </tr>
            <tr>
              <td>5</td>
              <td>1024x1024</td>
              <td>0.975385</td>
              <td>0.302306</td>
              <td>69.01%</td>
            </tr>
            <tr>
              <td>6</td>
              <td>1024x1024</td>
              <td>50.655735</td>
              <td>9.789422</td>
              <td>80.68%</td>
            </tr>
            <tr>
              <td>7</td>
              <td>1024x1024</td>
              <td>139.551437</td>
              <td>23.142061</td>
              <td>83.42%</td>
            </tr>
            <tr>
              <td>8</td>
              <td>1024x1024</td>
              <td>0.414549</td>
              <td>0.198096</td>
              <td>52.21%</td>
            </tr>
            <tr>
              <td>9</td>
              <td>1024x1024</td>
              <td>106.289078</td>
              <td>26.270395</td>
              <td>75.29%</td>
            </tr>
          </tbody>
        </table>

        <p>
          Adding a timer with
          <a href="https://en.cppreference.com/w/cpp/chrono" target="_blank"
            >std::chrono</a
          >
          early on was one of the cooler parts of working on this project.
          <strong>However, it was more than premature optimization</strong>;
          without introducing parallelization, I would not have been able to
          rapidly develop and render these scenes in time with my other
          coursework and responsibilities. Really nice to see prior knowledge
          from system courses (and at the time, Operating Systems) come into
          play.
        </p>

        <h2 id="bounding-volumes">Bounding Volumes</h2>
        <p>
          Comparatively speaking, scenes 6, 7, and 9 are clear outliers in
          render time. This is because the use of a mesh (the
          <a href="https://en.wikipedia.org/wiki/Stanford_bunny" target="_blank"
            >Stanford bunny</a
          >
          for these scenes), which is actually a
          <strong>collection of very many triangles</strong>. This "shape" is
          completely different from other shapes, which were overloaded with an
          intersection function called once for that shape.
        </p>

        <p>
          For a "mesh" shape, to determine if a ray intersects with the shape,
          you must check if it intersected with any of the triangles residing
          within the mesh. Problematically,
          <strong
            >we must always perform a linear search, iterating over every single
            triangle</strong
          >.
        </p>
        <p>
          Given that the bunny has 4968 triangles, and we have \(1024^2\) pixels
          or rays, we have to do \(1024^2 * 4968\) checks in the best case (no
          reflections).
          <strong>One check per ray, for every single triangle</strong>. This
          brings up the minimum bounding box problem, which is a different beast
          entirely.
        </p>

        <div class="project-img-container">
          <img
            src="./assets/boundingboxes.png"
            alt="Bounding Volume"
            class="project-img"
            style="width: 65%; height: auto"
          />
          Bounding volumes and tradeoffs, Ericson - Real-Time Collision
          Detection
        </div>

        <p>
          The main idea behind bounding volumes is to encapsulate a complex
          shape with a simpler one, which requires much less computation in the
          intersection test.
          <strong
            >If a ray doesn't even hit the bounding volume, then you certainly
            won't intersect with the mesh</strong
          >, so no need to perform the costly loop.
        </p>

        <p>
          Using a simple bounding sphere was sufficient for bringing down render
          time exponentially, but it is not the most efficient. Because I didn't
          research and implement a more advanced bounding volume technique,
          (i.e.
          <a href="https://en.wikipedia.org/wiki/Bounding_volume_hierarchy"
            >BVH</a
          >), the bunny was a limiting factor in render time for those scenes.
        </p>

        <h2 id="anti-aliasing">Anti-Aliasing</h2>
        <p>
          Throughout this writing I have mentioned the idea of shooting a ray
          through a pixel or through
          <strong>the "center" of the pixel</strong>. This is what normal ray
          tracing without anti-aliasing does.
        </p>
        <p>
          Anti-aliasing is described as the process of
          <strong>removing jagged edges along a shape's border</strong> that
          exist because we discretely map each pixel to one color from the
          scene. This does a poor job at handling differentiating the last pixel
          of a watermelon from the blue wall behind it.
        </p>

        <div class="project-img-container">
          <img
            src="./assets/1xAA.png"
            alt="Anti-Aliasing 1x"
            class="project-img"
            style="width: 120%; height: auto"
          />
          Scene 4, no anti-aliasing (1x)
        </div>

        <p>
          Instead, what if we argued the color for a pixel should be
          <strong
            >the weighted average of multiple rays that pass through the
            pixel?</strong
          >
          This is the fundamental idea behind this anti-aliasing - sampling a
          pixel with multiple rays to get a
          <strong>more accurate representation of its color</strong>.
        </p>

        <div class="project-img-container">
          <img
            src="./assets/mergedAA.jpg"
            alt="Anti-Aliasing 1x and 16x"
            class="project-img"
            style="width: 125%; height: auto"
          />
          <div>
            Scene 4, 512x512, no anti-aliasing (<strong>left</strong>) and 16x
            anti-aliasing (<strong>right</strong>)
          </div>
        </div>

        <p>
          As far as where you should shoot the now increased rays within a
          pixel, I chose to do it evenly throughout the pixel, but you can
          actually also do it completely randomly - the results are surprisingly
          good...
        </p>

        <h2 id="csg">Constructive Solid Geometry</h2>
        <div class="project-img-container">
          <img
            src="./assets/ybox.png"
            alt="Yellow Box"
            class="project-img-scene"
            style="width: 100%; height: 250px"
          />
          <div>Box with negative sphere</div>
        </div>

        <p>
          This is probably the most prominently <em>weird</em> part of this
          scene. It is clear what is going on here, but how is it possible?
          Well, there is a "box" shape with even sides, and inside that box is a
          sphere.
        </p>

        <p>
          But it is not any normal sphere like ones prior - it is one I denoted
          in my code as a "ShapeType::NEGATIVE" sphere. When my ray tracer sends
          a ray that contacts with any object that is of enum type "NEGATIVE",
          it
          <strong>effectively teleports through it</strong>.
        </p>

        <p>
          The reason I say <strong>effectively</strong>, is because it is a
          little weirder than that. What really happens is, the eye shoots a
          ray, it intersects closest at the <strong>entry</strong> point of the
          sphere, and it creates a new ray a <em>bit</em> further forward inside
          of my sphere to get the exit point of the sphere.
        </p>

        <p>
          We want the ray to basically ignore anything happening from the entry
          to the exit point of the negative shape. Here is a beautiful diagram
          of that occurring.
        </p>

        <div class="project-img-container">
          <img
            src="./assets/csgdemo.png"
            alt="CSG Diagram"
            class=""
            style="width: 100%; height: 350px"
          />
          <div>
            A beautiful diagram articulating the inner workings of my CSG
            implementation
          </div>
        </div>

        <p>
          So, the ray enters the negative sphere. In theory, it no longer
          exists; it cannot <strong>hit</strong> anything until it exits the
          sphere. As soon as it exits, however, it should interact with the
          first instance of a shape it can find. This creates the sphere imprint
          in the green cube of this image:
        </p>

        <div class="project-img-container">
          <img
            src="./assets/gbox.png"
            alt="Green Box"
            class="project-img-scene"
            style="width: 200px; height: auto"
          />
          Box with negative sphere and intersection
        </div>

        <p>
          To make my code support that idea, I would have had to refactor much
          of my code - mainly because it did not return the
          <strong>Hit()</strong> at the exit of the shape; only the enter. So
          instead of changing all of the code, I decided to create a new ray
          with an origin <strong>slightly</strong> further in the same direction
          of the original collision, and shoot that ray.
        </p>

        <p>
          Once that ray inevitably reached the exit of the sphere, there would
          be a hit! This hit was technically an "entrance" hit, but nonetheless
          it gave me the necessary data to create <strong>another</strong> new
          ray that would tell me what <em>actual</em> object lies on the other
          end of the negative shape, be it a box, ellipsoid, real sphere, etc...
        </p>

        <p>
          Perhaps not the most planned or systematic means of achieving
          constructive solid geometry, but I was shocked the idea worked - and
          consequently it created some cool effects nonetheless.
        </p>

        <h2 id="future">Future & Improvements</h2>
        <p>
          Here is a list of some goals I wanted to achieve for this project, but
          was unable to achieve to respect other obligations and course volume.
        </p>

        <ol>
          <li>
            Dynamic multithreading for certain use cases, i.e. mesh and triangle
            intersections
            <ol>
              <li>
                I reduced thread count in main and gave some to the triangle
                intersection code, but the overhead of creating threads for
                <code>MAX_THREADS / TRIANGLES</code> for each compute call is
                very expensive.
              </li>
              <li>
                I want to implement a producer worker thread pool that
                constructs these threads in a manner that optimizes the # of
                threads in scenes that are aware of many mesh instances.
              </li>
            </ol>
          </li>
          <li>
            More robust CSG implementation
            <ol>
              <li>
                I think my implementation works based on my own theory that
                hitting a "negative" shape teleports you through the negative
                shape, but I think overloaded new shapes that meaningfully and
                recursively join, subtract, or intersect would be a lot more
                compartmentalized.
              </li>
            </ol>
          </li>
          <li>
            Cook torrance & sampling of normals
            <ol>
              <li>
                I think Monte Carlo falls under this, but I want to learn how
                you can estimate surface material based on random distribution
                of normals as you hit the ground.
              </li>
            </ol>
          </li>
          <li>
            More efficient intersection (KD-trees, BSP-trees, etc...)
            <ol>
              <li>
                I have heard of KD trees and wonder how they work. I didn't try
                it for this assignment but think it would be cool to better
                contain the shape before making the expensive loop through all
                triangles in a mesh.
              </li>
              <li>
                This would in tandem with <code>1</code> do a lot for
                improvement of performance.
              </li>
            </ol>
          </li>
          <li>
            Skyboxes with texture mapping
            <ol>
              <li>
                I really want to do skyboxes. I have seen them in games and they
                look cool.
              </li>
            </ol>
          </li>
          <li>
            I want to add the idea of volumetric rays or godrays, might be Monte
            Carlo
            <ol>
              <li>
                I have explored this thought of dust particles which are
                attenuated by a light source, at the cost of brute force.
                Obviously I need to google it...
              </li>
            </ol>
          </li>
          <li>
            <del>
              Document all of my struggles! I need to go through my campuswire
              posts and make an archive of the journey here because I can't
              believe the destination.
            </del>
            <strong>Done</strong>
          </li>
        </ol>

        <h2 id="credits">Thanks & Credits</h2>
        <ol>
          <li>
            <a
              href="https://people.engr.tamu.edu/sueda/index.html"
              target="_blank"
              >Dr. Shinjiro Sueda</a
            >
          </li>
          <li>
            Michael Stewart's ray tracing
            <a
              href="https://michaelstewart2.github.io/RayTeaching1.1/index.html"
              target="_blank"
              >visualizer</a
            >
          </li>
          <li>
            My own notes & past code (including systems courses for threading),
            lecture slides.
          </li>
          <li>
            <a href="https://www.geeksforgeeks.org/cpp-polymorphism/?ref=lbp"
              >C++ Polymorphism</a
            >
          </li>
          <li>
            <a href="https://www.geeksforgeeks.org/virtual-function-cpp/"
              >Virtual Functions in C++</a
            >
          </li>
          <li>
            <a
              href="https://glm.g-truc.net/0.9.4/api/a00129.html#ga99b1ce602a02833646e5077345858da4"
              >glm::clamp</a
            >
          </li>
          <li>
            <a
              href="https://www.geeksforgeeks.org/measure-execution-time-function-cpp/"
              >std::chrono</a
            >
          </li>
          <li>
            <a
              href="https://fileadmin.cs.lth.se/cs/Personal/Tomas_Akenine-Moller/raytri/"
              >raytri.c</a
            >
          </li>
          <li>
            <a href="https://www.geeksforgeeks.org/multithreading-in-cpp/"
              >Threading</a
            >
          </li>
          <li>
            <a
              href="https://www.geeksforgeeks.org/mutex-lock-for-linux-thread-synchronization/"
              >Mutexing</a
            >
          </li>
          <li>
            <a href="https://en.cppreference.com/w/cpp/thread/thread"
              >std::thread</a
            >
          </li>
          <li>
            <a href="https://en.cppreference.com/w/cpp/thread/mutex"
              >std::mutex</a
            >
          </li>
          <li>
            <a href="https://tavianator.com/2011/ray_box.html"
              >Ray-Box Intersections</a
            >
          </li>
          <li>
            <a
              href="https://web.cse.ohio-state.edu/~parent.1/classes/681/Lectures/19.RayTracingCSG.pdf"
              >Constructive Solid Geometry Theory Slides</a
            >
          </li>
          <li>
            <a href="https://www.codecademy.com/resources/docs/markdown/tables"
              >Markdown Tables</a
            >
          </li>
        </ol>
      </section>

      <footer>
        <nav>
          <div class="nav-links-container">
            <ul class="nav-links">
              <li>
                <a href="../../">
                  <img
                    src="../../assets/icon-home.svg"
                    alt="home icon"
                    class="icon-grow"
                  />
                </a>
              </li>
              <li><a href="../../#about">me</a></li>
              <li><a href="../../#experience">experience</a></li>
              <li><a href="../../#contact">contact</a></li>
              <li><a href="../../pages/projects.html">projects</a></li>
              <li><a href="../../pages/fun.html">fun</a></li>
            </ul>
          </div>
        </nav>
        <p>&#169; 2024 Andrew Leach. All Rights Reserved.</p>
      </footer>

      <script src="../../script.js" async defer></script>
    </html>
  </body>
</html>
